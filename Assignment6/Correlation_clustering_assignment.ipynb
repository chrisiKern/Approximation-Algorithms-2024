{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mgo9yFR5gk53"
   },
   "source": [
    "This colab contains code for creating a correlation clustering problem defined by two weight matrices, W_plus and W_minus. You should add code to implement an approximation algorithm using semidefinite programming and randomized rounding, as described in [Williamson and Shmoys](https://www.designofapproxalgs.com/book.pdf) section 6.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDP9zhhSgbpN"
   },
   "source": [
    "# Construct weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch and import libraries\n",
    "import picos as pc\n",
    "import cvxopt as cvx\n",
    "import cvxopt.lapack\n",
    "from scipy.linalg import cholesky\n",
    "import numpy as np\n",
    "import itertools as it"
   ],
   "metadata": {
    "id": "qnLruc9_PxZZ",
    "ExecuteTime": {
     "end_time": "2024-05-28T13:29:56.074999Z",
     "start_time": "2024-05-28T13:29:54.886839Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yloKd7up4v54",
    "ExecuteTime": {
     "end_time": "2024-05-28T13:32:43.017646Z",
     "start_time": "2024-05-28T13:32:41.846897Z"
    }
   },
   "source": [
    "# Fetch data\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/denmark-0.6.txt -o ./data/denmark-0.6.txt\n",
    "!curl https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/learning-0.6.txt -o ./data/learning-0.6.txt\n",
    "!curl https://raw.githubusercontent.com/rasmus-pagh/apx/main/data/copenhagen-0.5.txt -o ./data/copenhagen-0.5.txt\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  103k  100  103k    0     0   368k      0 --:--:-- --:--:-- --:--:--  369k\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  162k  100  162k    0     0  1317k      0 --:--:-- --:--:-- --:--:-- 1321k\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  323k  100  323k    0     0  1220k      0 --:--:-- --:--:-- --:--:-- 1217k\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsIhC-oYhcBm"
   },
   "source": [
    "There are three data files containing [GloVe](https://nlp.stanford.edu/projects/glove/) vectors, whose dot products measure the similarity between words. The matrix W_plus is defined as dot products of such vectors, while W_minus is constant, equaling the average in W_plus."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eMSxFYXLTCH7",
    "ExecuteTime": {
     "end_time": "2024-05-29T06:48:37.767225Z",
     "start_time": "2024-05-29T06:48:37.761509Z"
    }
   },
   "source": [
    "# Choose one of the three data files\n",
    "filename = \"./data/\" + ['learning-0.6.txt', 'copenhagen-0.5.txt', 'denmark-0.6.txt'][2]\n",
    "\n",
    "# Read vectors and construct matrices\n",
    "with open(filename, 'r') as f:\n",
    "  feature_vectors = []\n",
    "  words = []\n",
    "  for line in f:\n",
    "    word, vector = line.split(';')\n",
    "    words.append(word)\n",
    "    vector = [ float(x) for x in vector.split(',') ]\n",
    "    feature_vectors.append(vector)\n",
    "  n = len(words)\n",
    "  feature_vectors = np.array(feature_vectors)\n",
    "  W_plus = np.dot(feature_vectors, np.transpose(feature_vectors))\n",
    "  W_minus = np.ones(shape=(n,n)) * np.average(W_plus)"
   ],
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr2aTUfNotXa"
   },
   "source": [
    "# Correlation clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqHOXaPSiNg6"
   },
   "source": [
    "Here you can implement your approximation algorithm. It may be helpful to consult the [implementation](https://colab.research.google.com/drive/1Rhe0kra6mqt5VHc2uTlNzJ_JC6kpG8nA?usp=sharing) of an approximation algorithm for Max Cut. Your implementation must:\n",
    "- Define and solve the semidefinite programming relaxation\n",
    "- Output the upper bound on OPT given by the relaxation\n",
    "- Output the expected value of the objective function with a random 4-clustering\n",
    "- Output the value of the best 4-clustering found using randomized rounding (say, in 100 trials), and the words placed in each cluster.\n",
    "\n",
    "If you experience problems with convergence (optimizer not terminating) don't worry about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cA6kIO8Fiotu",
    "ExecuteTime": {
     "end_time": "2024-05-29T11:14:13.532935Z",
     "start_time": "2024-05-29T11:14:02.649458Z"
    }
   },
   "source": [
    "correlation_clustering = pc.Problem()\n",
    "\n",
    "# Define variables and constants\n",
    "X = pc.SymmetricVariable('X',(n,n))\n",
    "W_p = pc.Constant('W_p', W_plus)\n",
    "W_m = pc.Constant('W_m', W_minus)\n",
    "ones = pc.Constant('ones', np.ones((n,n)))\n",
    "\n",
    "# Define objective and constraints\n",
    "correlation_clustering.set_objective('max', (W_p | X) + (W_m | (ones - X)))\n",
    "correlation_clustering.add_constraint(pc.maindiag(X) == 1)\n",
    "correlation_clustering.add_constraint(X >= 0)\n",
    "correlation_clustering.add_constraint(X >> 0)\n",
    "\n",
    "# Solve the problem\n",
    "correlation_clustering.solve(solver='cvxopt', verbosity=1)\n",
    "V = cholesky(X.value + 1e-3 * np.identity(n))"
   ],
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "           PICOS 2.4.17            \n",
      "===================================\n",
      "Problem type: Semidefinite Program.\n",
      "Searching a solution strategy for CVXOPT.\n",
      "Solution strategy:\n",
      "  1. ExtraOptions\n",
      "  2. CVXOPTSolver\n",
      "Applying ExtraOptions.\n",
      "Building a CVXOPT problem instance.\n",
      "Starting solution search.\n",
      "-----------------------------------\n",
      " Python Convex Optimization Solver \n",
      "    via internal CONELP solver     \n",
      "-----------------------------------\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0: -2.1196e+01 -2.1196e+01  8e+03  5e+01  2e+01  1e+00\n",
      " 1: -2.8263e+01 -2.8344e+01  2e+03  3e+01  1e+01  5e-01\n",
      " 2: -5.5176e+01 -5.4808e+01  8e+02  8e+00  3e+00  5e-01\n",
      " 3: -7.2168e+01 -7.1971e+01  4e+02  4e+00  2e+00  3e-01\n",
      " 4: -9.1143e+01 -9.1095e+01  2e+02  2e+00  7e-01  8e-02\n",
      " 5: -9.8230e+01 -9.8206e+01  8e+01  1e+00  4e-01  4e-02\n",
      " 6: -1.0469e+02 -1.0468e+02  3e+01  4e-01  2e-01  1e-02\n",
      " 7: -1.0637e+02 -1.0637e+02  2e+01  2e-01  9e-02  9e-03\n",
      " 8: -1.0780e+02 -1.0780e+02  1e+01  1e-01  5e-02  5e-03\n",
      " 9: -1.0862e+02 -1.0862e+02  6e+00  7e-02  3e-02  3e-03\n",
      "10: -1.0933e+02 -1.0933e+02  2e+00  3e-02  1e-02  9e-04\n",
      "11: -1.0957e+02 -1.0957e+02  1e+00  2e-02  7e-03  5e-04\n",
      "12: -1.0974e+02 -1.0974e+02  5e-01  6e-03  3e-03  2e-04\n",
      "13: -1.0981e+02 -1.0981e+02  2e-01  2e-03  1e-03  7e-05\n",
      "14: -1.0983e+02 -1.0983e+02  1e-01  1e-03  5e-04  4e-05\n",
      "15: -1.0984e+02 -1.0984e+02  4e-02  6e-04  2e-04  2e-05\n",
      "16: -1.0985e+02 -1.0985e+02  1e-02  2e-04  7e-05  5e-06\n",
      "17: -1.0985e+02 -1.0985e+02  4e-03  5e-05  2e-05  1e-06\n",
      "18: -1.0985e+02 -1.0985e+02  2e-03  3e-05  1e-05  8e-07\n",
      "19: -1.0985e+02 -1.0985e+02  7e-04  1e-05  4e-06  3e-07\n",
      "20: -1.0985e+02 -1.0985e+02  2e-04  3e-06  1e-06  8e-08\n",
      "21: -1.0985e+02 -1.0985e+02  1e-04  1e-06  5e-07  4e-08\n",
      "22: -1.0985e+02 -1.0985e+02  5e-06  7e-08  3e-08  2e-09\n",
      "23: -1.0985e+02 -1.0985e+02  5e-07  6e-09  3e-09  2e-10\n",
      "Optimal solution found.\n",
      "------------[ CVXOPT ]-------------\n",
      "Solver claims optimal solution for feasible problem.\n",
      "Applying the solution.\n",
      "Applied solution is primal feasible.\n",
      "Search 1.1e+01s, solve 1.1e+01s, overhead 0%.\n",
      "=============[ PICOS ]=============\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound of OPT given by the relaxation: 1629.8608433121885\n",
      "Expected value of random 4-clustering: 1520.0115731912529\n"
     ]
    }
   ],
   "source": [
    "print(f\"Upper bound of OPT given by the relaxation: {correlation_clustering.value}\")\n",
    "print(f\"Expected value of random 4-clustering: {1/4 * np.sum(W_plus) + 3/4 * np.sum(W_minus)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T06:58:57.395123Z",
     "start_time": "2024-05-29T06:58:57.389677Z"
    }
   },
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster weight found by randomized rounding: 1623.067219764774 (best) and 1515.6005888133845 (worst)\n",
      "Clusters of best 4-clustering found by randomized rounding:\n",
      "\tCluster 1: ['brisbane' 'newcastle' 'melbourne' 'nz' 'australia' 'wales' 'london'\n",
      " 'sydney' 'cardiff' 'england']\n",
      "\tCluster 2: ['switzerland' 'belgium' 'finland' 'slovakia' 'holland' 'zealand' 'russia'\n",
      " 'bulgaria' 'czech' 'serbia' 'denmark' 'austria' 'lithuania' 'germany'\n",
      " 'slovenia' 'italy' 'norway' 'ireland' 'croatia' 'brazil' 'spain' 'greece'\n",
      " 'sweden' 'netherlands' 'latvia' 'romania' 'poland' 'europe' 'hungary'\n",
      " 'iceland' 'belarus' 'azerbaijan' 'kazakhstan' 'estonia' 'morocco']\n",
      "\tCluster 3: ['netherland' 'englan']\n",
      "\tCluster 4: ['belanda' 'jerman' 'thailand' 'portugal']\n"
     ]
    }
   ],
   "source": [
    "partition_weights = []\n",
    "partitions = []\n",
    "repetitions = 10**5\n",
    "\n",
    "# Perform randomized rounding many times, compute the cluster weight each time\n",
    "for _ in range(repetitions):\n",
    "  # Generate two random hyperplanes and compute dot product with V\n",
    "  R = np.random.normal(size = (2, n))\n",
    "  R_dot_V = np.dot(R, V)\n",
    "  \n",
    "  # Compute the vertices of each cluster\n",
    "  P = [(R_dot_V[0] >= 0) * (R_dot_V[1] >= 0), \n",
    "       (R_dot_V[0] >= 0) * (R_dot_V[1] < 0), \n",
    "       (R_dot_V[0] < 0) * (R_dot_V[1] >= 0),\n",
    "       (R_dot_V[0] < 0) * (R_dot_V[1] < 0)]\n",
    "  \n",
    "  # Compute the words for each cluster\n",
    "  partitions.append([np.array(words)[c] for c in P])\n",
    "  \n",
    "  # Compute the edges within clusters (plus_edges) and between clusters (minus_edges)\n",
    "  plus_edges = np.sum([np.outer(c, c) for c in P], axis=0)\n",
    "  minus_edges = 1 - plus_edges  \n",
    "  \n",
    "  # Compute the cluster weight\n",
    "  partition_weights.append(np.sum(W_plus * plus_edges + W_minus * minus_edges))\n",
    "  \n",
    "\n",
    "print(f\"Cluster weight found by randomized rounding: {max(partition_weights)} (best) and {min(partition_weights)} (worst)\")\n",
    "partition = partitions[np.argmax(partition_weights)]\n",
    "print(f\"Clusters of best 4-clustering found by randomized rounding:\")\n",
    "for i in range(4):\n",
    "  print(f\"\\tCluster {i+1}: {partition[i]}\")\n",
    "  \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T13:26:56.396343Z",
     "start_time": "2024-05-29T13:26:49.369539Z"
    }
   },
   "execution_count": 253
  }
 ]
}
